{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALANDO DEPENDÊNCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instalando módulos novos\n",
    "! pip install numpy\n",
    "! pip install sklearn\n",
    "! pip install tensorflow\n",
    "! pip install tflearn\n",
    "#! pip install pypiwin32\n",
    "! pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações Iniciais\n",
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definição de variaveis \n",
    "FILES_DIR = 'news'\n",
    "FILES_DIR_UNKNOW = 'desconhecidos'\n",
    "MINIMO_LETRAS = 3\n",
    "PALAVRAS_IGNORADAS = ['@', '.', '!','?',',','$','-','\\'s','g','(',')','[',']',':','\\n']\n",
    "NUMERO_PALAVRAS_MAIS_FREQUENTES = 1000\n",
    "PERCENTUAL_TESTE = 0.33\n",
    "PASSOS_TREINAMENTO = 100\n",
    "BATCH_SIZE = 50\n",
    "VALIDATION_SET = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANDO DATASET E NOTICIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "categorias = []\n",
    "documentos = []\n",
    "resultados = []\n",
    "i = 0\n",
    "for _, dirs, _ in os.walk(os.path.join(FILES_DIR)):\n",
    "    for d in dirs:\n",
    "        if d not in categorias:\n",
    "            categorias.append(d)\n",
    "        print('lendo categoria {}'.format(d))\n",
    "        for _, _, files in os.walk(os.path.join(FILES_DIR,d)):\n",
    "            for f in files:\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print('{} arquivos lidos. lendo {}'.format(i,f))\n",
    "                with codecs.open(os.path.join(FILES_DIR,d,f),'r',encoding=\"utf8\", errors='ignore') as f:                     \n",
    "                     documentos.append(f.read())\n",
    "                resultados.append(d)\n",
    "end = time.time()                \n",
    "print ('\\nExistem {} textos e {} categorias: {}. \\nProcessamento em {}s'.format(len(documentos),len(categorias),categorias,end - start))                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMATAÇÃO DOS INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "documentos_tokenizados = []\n",
    "for documento in documentos:\n",
    "    documentos_tokenizados.append(word_tokenize(documento))    \n",
    "end = time.time()\n",
    "print ('{} documentos tokenizados. \\nProcessamento em {}s'.format(len(documentos_tokenizados),end - start))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos_tokenizados[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ptbr_stops = set(stopwords.words('portuguese'))\n",
    "documentos_sem_stop_words_e_ignore = []\n",
    "for documento in documentos_tokenizados:    \n",
    "    aux = [w for w in documento if w not in ptbr_stops]   \n",
    "    aux2 = [w for w in aux if w not in PALAVRAS_IGNORADAS]    \n",
    "    documentos_sem_stop_words_e_ignore.append([w for w in aux2 if len(w) >= MINIMO_LETRAS])\n",
    "end = time.time()\n",
    "print ('{} documentos removidos as stop words e palavras ignoradas. \\nProcessamento em {}s'\n",
    "       .format(len(documentos_sem_stop_words_e_ignore),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos_sem_stop_words_e_ignore[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processando o stemming para cada palavra, transformando em minísculo\n",
    "start = time.time()\n",
    "stemmer = SnowballStemmer('portuguese')\n",
    "documentos_apos_stemming = []\n",
    "for documento in documentos_sem_stop_words_e_ignore:\n",
    "    # List comprehension para fazer o stemming de cada palavra\n",
    "    documentos_apos_stemming.append([stemmer.stem(w.lower()) for w in documento])\n",
    "end = time.time()\n",
    "print ('{} documentos após aplicar stemmer. \\nProcessamento em {}s'.format(len(documentos_apos_stemming),end - start))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento após processamento\n",
    "print(documentos_apos_stemming[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "todas_palavras = []\n",
    "for documento in documentos_apos_stemming:\n",
    "    todas_palavras.extend(documento)\n",
    "end = time.time()\n",
    "print ('foram reunidas um todal de {} palavras. \\nProcessamento em {}s'.format(len(todas_palavras),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "fdist = nltk.FreqDist(todas_palavras)\n",
    "palavras_mais_frequentes = []\n",
    "for word_freq in fdist.most_common(NUMERO_PALAVRAS_MAIS_FREQUENTES):\n",
    "    palavras_mais_frequentes.append(word_freq[0])\n",
    "palavras_mais_frequentes = sorted(list(set(palavras_mais_frequentes)))\n",
    "end = time.time()\n",
    "print('Número total de tokens distintos {}. \\nSelecionando apenas as {} palavras mais frequentes. \\nProcessamento em {}s'\n",
    "      .format(fdist.N(),NUMERO_PALAVRAS_MAIS_FREQUENTES,end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(palavras_mais_frequentes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "documentos_bow = []\n",
    "resultados_bow = []\n",
    "for i in range(len(documentos_apos_stemming)):\n",
    "    documento = documentos_apos_stemming[i]\n",
    "    resultado = resultados[i]   \n",
    "    documento_bow = []\n",
    "    # Cria um array com o bag of words de N palavras mais frequentes, colocando 1 quando a palavra estiver presente no documento\n",
    "    for w in palavras_mais_frequentes:\n",
    "        documento_bow.append(1) if w in documento else documento_bow.append(0)\n",
    "    documentos_bow.append(documento_bow)\n",
    "    # Inicializa o array de resultados com zero    \n",
    "    resultado_bow = list([0] * len(categorias))\n",
    "    # Coloca 1 na posição do array correta de acordo com o resultado   \n",
    "    resultado_bow[categorias.index(resultado)] = 1\n",
    "    resultados_bow.append(resultado_bow)\n",
    "end = time.time()\n",
    "print('Criou um total de {} documentos BoW. \\nProcessamento em {}s'.format(len(documentos_bow),end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento BoW\n",
    "print('primeiro documento {}'.format(documentos_bow[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimindo o primeiro documento BoW\n",
    "print(len(resultados_bow))\n",
    "print('primeiro resultado BoW {} para as categorias {}'.format(resultados_bow[1], categorias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_bow = np.array(documentos_bow)\n",
    "resultados_bow = np.array(resultados_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTE VS TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documentos_bow, resultados_bow, test_size=PERCENTUAL_TESTE, random_state=42)\n",
    "print('Separando {} documentos/resultados para treinamento e {} para teste'.format(len(X_train),len(X_test)))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOSSA REDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(y_train[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "# Define o modelo e configura o tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs/'+time.strftime(\"%Y%m%d_%H%M\"))\n",
    "print('Modelo de rede neural criado: {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando o Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, n_epoch=PASSOS_TREINAMENTO, batch_size=BATCH_SIZE, validation_set=VALIDATION_SET, show_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURÁCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuarcy: %0.4f%%' % (score[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "prediction = model.predict([X_test[4]])\n",
    "print(\"Valor previsto: {}. \\nValor esperado: {}\".format(prediction[0],y_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VALIDANDO NA PRÁTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova = ''\n",
    "nova_token = []\n",
    "nova_no_stop = []\n",
    "nova_stemming = []\n",
    "novas_bow = []\n",
    "with open(os.path.join(FILES_DIR_UNKNOW,'noticia1.txt'),'r',encoding=\"utf8\", errors='ignore') as f: \n",
    "                  nova = f.read()\n",
    "                   \n",
    "#------------------------------------------lido nova------------------------------------------------'         \n",
    "\n",
    "nova_token=word_tokenize(nova)\n",
    "\n",
    "#'--------------------------------------------token---------------------------------------------------'\n",
    "\n",
    "\n",
    "aux = [w for w in nova_token if w not in ptbr_stops]   \n",
    "aux2 = [w for w in aux if w not in PALAVRAS_IGNORADAS]    \n",
    "nova_no_stop.append([w for w in aux2 if len(w) >= MINIMO_LETRAS])\n",
    "\n",
    "#'-------------------------------------------steammin------------------------------------------------'\n",
    "\n",
    "nova_stemming.append([stemmer.stem(w.lower()) for w in nova_no_stop[0]])\n",
    "\n",
    "\n",
    "#'-------------------------------------------bag-------------------------------------------------------'\n",
    "\n",
    "for w in palavras_mais_frequentes:\n",
    "        novas_bow.append(1) if w in nova_stemming[0] else novas_bow.append(0)\n",
    "    \n",
    "prediction = model.predict([X_test[1]])\n",
    "print(\"Valor previsto: {}. \\nValor esperado: {}\".format(prediction[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
